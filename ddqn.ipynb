{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e src/gym-idsgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping gymnasium as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gym==0.21.0\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'tests_require'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting gym-idsgame==1.0.12\n",
      "  Using cached gym_idsgame-1.0.12-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting gym (from gym-idsgame==1.0.12)\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Collecting pyglet (from gym-idsgame==1.0.12)\n",
      "  Using cached pyglet-2.1.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from gym-idsgame==1.0.12) (2.2.4)\n",
      "Collecting torch (from gym-idsgame==1.0.12)\n",
      "  Using cached torch-2.6.0-cp313-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting matplotlib (from gym-idsgame==1.0.12)\n",
      "  Using cached matplotlib-3.10.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from gym-idsgame==1.0.12)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting tqdm (from gym-idsgame==1.0.12)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting opencv-python (from gym-idsgame==1.0.12)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting imageio (from gym-idsgame==1.0.12)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jsonpickle (from gym-idsgame==1.0.12)\n",
      "  Using cached jsonpickle-4.0.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting tensorboard (from gym-idsgame==1.0.12)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sklearn (from gym-idsgame==1.0.12)\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip uninstall gymnasium\n",
    "!pip install gym==0.21.0\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install gym-idsgame==1.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium.core import Env\n",
    "\n",
    "def patched_reset(self):\n",
    "    return self.reset()\n",
    "\n",
    "Env.reset = patched_reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE IMPORTING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import numpy as np\n",
    "import torch\n",
    "# src/environment/idsgame_wrapper.py\n",
    "# from src.agents.dqn_agent import DQNAgent\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.utils import print_summary\n",
    "from src.utils.plotting import plot_results\n",
    "# from src.utils import create_artefact_dirs\n",
    "\n",
    "print('DONE IMPORTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from src.environment.explorer import IDSGameExplorer\n",
    "explorer = IDSGameExplorer()\n",
    "# explorer.run_comprehensive_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishnaasrinivas/Desktop/AI-Agent-for-Cyber-Security/src/agents/ddqn_agent.py\n"
     ]
    }
   ],
   "source": [
    "import src.agents.ddqn_agent\n",
    "print(src.agents.ddqn_agent.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# from gym_idsgame.agents.training_agents.q_learning.q_agent_config import QAgentConfig\n",
    "# from gym_idsgame.agents.training_agents.q_learning.dqn.dqn_config import DQNConfig\n",
    "from experiments.util import util\n",
    "from experiments.util.plotting_util import read_and_plot_results\n",
    "# from src.agents.ddqn_agent import DDQNAgent\n",
    "from src.utils.utils import get_output_dir, print_summary\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.plotting import plot_results\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment Information:\n",
      "Observation Space: Box(0, 9, (1, 11), int32)\n",
      "Action Space: Discrete(30)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"idsgame-random_attack-v8\"\n",
    "output_dir = os.getcwd()\n",
    "random_seed = 42\n",
    "env = gym.make(env_name, save_dir=output_dir + \"results/data/\" + str(random_seed))\n",
    "# env = GymCompatibilityWrapper(env)\n",
    "\n",
    "env = GymCompatibilityWrapper(env)\n",
    "env = env.unwrapped\n",
    "\n",
    "print(\"\\nEnvironment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: idsgame-random_attack-v8\n",
      "Action space: Discrete(30)\n",
      "Observation space: Box(0, 9, (1, 11), int32)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create output directory for results\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "result_dir = f\"./results/ddqn_pytorch_{current_time}\"\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "# # Create environment\n",
    "# env_name = \"idsgame-random_defense-v0\"\n",
    "# env = gym.make(env_name, save_dir=result_dir)\n",
    "\n",
    "# Print environment information\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state dimension: 30\n",
      "Agent action dimension: 30\n",
      "Neural network hidden layers: [128, 128]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "\n",
    "# Configure DDQN agent\\n\",\n",
    "ddqn_config = DDQNConfig(\n",
    "       gamma=0.99,                       # Discount factor\\n\",\n",
    "        lr=0.00005,                       # Learning rate\\n\",\n",
    "        batch_size=64,                    # Batch size for training\\n\",\n",
    "      epsilon=1.0,                      # Initial exploration rate\\n\",\n",
    "       epsilon_decay=0.999,              # Decay rate for epsilon\\n\",\n",
    " min_epsilon=0.01,                 # Minimum value for epsilon\\n\",\n",
    "   target_network_update_freq=100,   # Frequency of target network updates\\n\",\n",
    "    replay_memory_size=10000,         # Size of replay buffer\\n\",\n",
    "   num_episodes=5000,                # Number of episodes to train for\\n\",\n",
    "  eval_frequency=500,               # Frequency of evaluations during training\\n\",\n",
    "  eval_episodes=50,                 # Number of episodes for each evaluation\\n\",\n",
    "  train_log_frequency=50,           # Frequency of logging during training\\n\",\n",
    "  eval_log_frequency=1,             # Frequency of logging during evaluation\\n\",\n",
    " eval_render=False,                # Whether to render during evaluation\\n\",\n",
    " render=False,                     # Whether to render during training\\n\",\n",
    " attacker=True,                    # Whether the agent is an attacker\\n\",\n",
    "defender=False,                   # Whether the agent is a defender\\n\",\n",
    " save_dir=result_dir,              # Directory to save results\\n\",\n",
    " save_frequency=500,               # Frequency to save the model during training\\n\",\n",
    " hidden_layers=[128, 128],         # Hidden layer sizes for the neural network\\n\",\n",
    " device=device                     # Device to run the neural network on\\n\",\n",
    "    )\n",
    "agent = DDQNAgent(env, ddqn_config)\n",
    "\n",
    "# Print agent information\n",
    "print(f\"Agent state dimension: {agent.state_dim}\")\n",
    "print(f\"Agent action dimension: {agent.num_actions}\")\n",
    "print(f\"Neural network hidden layers: {ddqn_config.hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(result, title=\"\"):\n",
    "    print(f\"\\n{title} Summary:\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"- Average Reward: {np.mean(result.episode_rewards):.2f} ± {np.std(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Max Reward: {np.max(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Min Reward: {np.min(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Avg Episode Length: {np.mean(result.episode_steps):.2f} steps\")\n",
    "\n",
    "    # Call this after training\n",
    "    print_summary(agent.train_result, \"Training\")\n",
    "    print_summary(agent.eval_result, \"Evaluation\")\n",
    "\n",
    "    print(f\"- Cumulative Reward: {sum(result.episode_rewards)}\")\n",
    "\n",
    "def plot_training(agent):\n",
    "    \"\"\"Plot training progress\"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Smoothed rewards\n",
    "    rewards = agent.train_result.episode_rewards\n",
    "    window_size = 50\n",
    "    smooth_rewards = [np.mean(rewards[max(0,i-window_size):i+1]) \n",
    "                     for i in range(len(rewards))]\n",
    "    \n",
    "    plt.plot(smooth_rewards)\n",
    "    plt.title(\"Training Progress (Smoothed Rewards)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Environment: idsgame-random_attack-v8\n",
      "State dimension: 33\n",
      "Action dimension: 30\n",
      "Starting training for 20000 episodes...\n",
      "[Ep 1/20000] AvgReward (last 1): -1.00 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 0.0s\n",
      "[Ep 500/20000] AvgReward (last 500): -0.12 | Total: -1.00 | Epsilon: 1.000 | Steps: 6 | Time: 0.7s\n",
      "[Ep 1000/20000] AvgReward (last 500): 0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 1.3s\n",
      "[Ep 1500/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 1.8s\n",
      "[Ep 2000/20000] AvgReward (last 500): -0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 2.4s\n",
      "[Ep 2500/20000] AvgReward (last 500): 0.03 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 3.0s\n",
      "[Ep 3000/20000] AvgReward (last 500): -0.09 | Total: 0.20 | Epsilon: 1.000 | Steps: 9 | Time: 3.5s\n",
      "[Ep 3500/20000] AvgReward (last 500): -0.06 | Total: 0.12 | Epsilon: 1.000 | Steps: 10 | Time: 4.1s\n",
      "[Ep 4000/20000] AvgReward (last 500): -0.03 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 4.7s\n",
      "[Ep 4500/20000] AvgReward (last 500): -0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 5.2s\n",
      "[Ep 5000/20000] AvgReward (last 500): -0.16 | Total: 2.23 | Epsilon: 1.000 | Steps: 20 | Time: 5.8s\n",
      "[Ep 5500/20000] AvgReward (last 500): -0.01 | Total: 0.50 | Epsilon: 1.000 | Steps: 7 | Time: 6.4s\n",
      "[Ep 6000/20000] AvgReward (last 500): 0.02 | Total: 3.00 | Epsilon: 1.000 | Steps: 6 | Time: 7.0s\n",
      "[Ep 6500/20000] AvgReward (last 500): 0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 7.7s\n",
      "[Ep 7000/20000] AvgReward (last 500): 0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 8.3s\n",
      "[Ep 7500/20000] AvgReward (last 500): -0.10 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 8.9s\n",
      "[Ep 8000/20000] AvgReward (last 500): 0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 9.4s\n",
      "[Ep 8500/20000] AvgReward (last 500): -0.11 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 10.0s\n",
      "[Ep 9000/20000] AvgReward (last 500): -0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 10.6s\n",
      "[Ep 9500/20000] AvgReward (last 500): -0.06 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 11.2s\n",
      "[Ep 10000/20000] AvgReward (last 500): 0.03 | Total: 3.33 | Epsilon: 1.000 | Steps: 5 | Time: 11.7s\n",
      "[Ep 10500/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 12.3s\n",
      "[Ep 11000/20000] AvgReward (last 500): 0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 12.9s\n",
      "[Ep 11500/20000] AvgReward (last 500): 0.06 | Total: 2.42 | Epsilon: 1.000 | Steps: 12 | Time: 13.5s\n",
      "[Ep 12000/20000] AvgReward (last 500): 0.13 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 14.1s\n",
      "[Ep 12500/20000] AvgReward (last 500): 0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 14.7s\n",
      "[Ep 13000/20000] AvgReward (last 500): -0.10 | Total: 0.25 | Epsilon: 1.000 | Steps: 10 | Time: 15.2s\n",
      "[Ep 13500/20000] AvgReward (last 500): -0.02 | Total: 0.33 | Epsilon: 1.000 | Steps: 5 | Time: 15.8s\n",
      "[Ep 14000/20000] AvgReward (last 500): -0.03 | Total: 1.00 | Epsilon: 1.000 | Steps: 7 | Time: 16.3s\n",
      "[Ep 14500/20000] AvgReward (last 500): -0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 16.9s\n",
      "[Ep 15000/20000] AvgReward (last 500): -0.01 | Total: 0.12 | Epsilon: 1.000 | Steps: 12 | Time: 17.5s\n",
      "[Ep 15500/20000] AvgReward (last 500): -0.07 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 18.1s\n",
      "[Ep 16000/20000] AvgReward (last 500): -0.09 | Total: 3.33 | Epsilon: 1.000 | Steps: 5 | Time: 18.7s\n",
      "[Ep 16500/20000] AvgReward (last 500): -0.13 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 19.4s\n",
      "[Ep 17000/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 20.0s\n",
      "[Ep 17500/20000] AvgReward (last 500): 0.04 | Total: 0.33 | Epsilon: 1.000 | Steps: 8 | Time: 20.6s\n",
      "[Ep 18000/20000] AvgReward (last 500): -0.15 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 21.1s\n",
      "[Ep 18500/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 7 | Time: 21.7s\n",
      "[Ep 19000/20000] AvgReward (last 500): 0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 22.2s\n",
      "[Ep 19500/20000] AvgReward (last 500): -0.02 | Total: 0.50 | Epsilon: 1.000 | Steps: 6 | Time: 22.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n",
      "2025-04-11 04:14:25,809 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_04-14-02/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 20000/20000] AvgReward (last 500): -0.11 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 23.4s\n",
      "\n",
      "Training completed in 23.4 seconds\n",
      "\n",
      "📊 Final DDQN Training Performance:\n",
      "idsgame-random_attack-v8\n",
      "Results:  {'average_reward': -0.03574216806885925, 'reward_std': 1.461620471100011, 'max_reward': 4.0, 'min_reward': -1.0, 'average_episode_length': 4.6574, 'episode_length_std': 3.588735883288153, 'max_episode_length': 28, 'min_episode_length': 1, 'average_hack_probability': 0.0, 'hack_probability_std': 0.0, 'max_hack_probability': 0.0, 'min_hack_probability': 0.0, 'cumulative_reward': -714}\n",
      "- Average Reward: -0.04 ± 1.46\n",
      "- Max-Min Reward: 4.00 - -1.00\n",
      "- Average Episode Length: 4.66 ± 3.59\n",
      "- Cumulative Reward: -714\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def extract_attacker_obs(obs):\n",
    "    \"\"\"Extract and flatten attacker observation\"\"\"\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Handle gym>=0.26.0 reset() returning (obs, info)\n",
    "    return np.array(obs).flatten()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # Create environment\n",
    "# env_name = \"idsgame-minimal_defense-v2\"\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "result_dir = f\"./results/ddqn_{timestamp}\"\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "sample_obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "\n",
    "# Create DDQN configuration\n",
    "config = DDQNConfig(\n",
    "    gamma=0.99,                       # Discount factor\n",
    "    lr=1e-3,                          # Learning rate\n",
    "    batch_size=32,                    # Reduce batch size to improve stability\n",
    "    epsilon=1.0,                      # Initial exploration rate\n",
    "    epsilon_decay=0.995,              # Decay rate for epsilon\n",
    "    min_epsilon=0.01,                 # Minimum value for epsilon\n",
    "    target_network_update_freq=10,    # Update target network frequency\n",
    "    replay_memory_size=10000,         # Size of replay buffer\n",
    "    num_episodes=10000,               # Number of episodes to train for\n",
    "    train_log_frequency=100,          # Log progress every 100 episodes\n",
    "    eval_frequency=1000,              # Evaluate every 1000 episodes\n",
    "    save_dir=result_dir,              # Directory to save results\n",
    "    hidden_layers=[64, 64]            # Smaller hidden layers for stability\n",
    ")\n",
    "agent = DDQNAgent(env, config)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 20000\n",
    "max_steps = 100\n",
    "log_frequency = 500\n",
    "\n",
    "# For tracking progress\n",
    "reward_history = []\n",
    "episode_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for ep in range(num_episodes):\n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]\n",
    "        \n",
    "    state = extract_attacker_obs(obs)  # Make sure state is properly formatted\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_steps):\n",
    "        # Select attacker action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Get defender action (random)\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        \n",
    "        # Combine actions for the environment\n",
    "        full_action = (action, defense_action)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_obs, reward, terminated, truncated, info = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Get attacker reward\n",
    "        attacker_reward = reward[0] if isinstance(reward, tuple) else reward\n",
    "        \n",
    "        # Extract and format next state\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.replay_buffer.push(state, action, attacker_reward, next_state, bool(done))\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += attacker_reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Store episode metrics\n",
    "    reward_history.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Log progress\n",
    "    if (ep + 1) % log_frequency == 0 or ep == 0:\n",
    "        avg_reward = np.mean(reward_history[-min(log_frequency, len(reward_history)):]) \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Ep {ep+1}/{num_episodes}] AvgReward (last {min(log_frequency, len(reward_history))}): {avg_reward:.2f} | \"\n",
    "              f\"Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | \"\n",
    "              f\"Steps: {steps} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "# Agent training should have been updating automatically through the \n",
    "# agent's internal train() method, if implemented properly\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": np.mean(episode_lengths),\n",
    "    \"episode_length_std\": np.std(episode_lengths),\n",
    "    \"max_episode_length\": np.max(episode_lengths),\n",
    "    \"min_episode_length\": np.min(episode_lengths),\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"\\n📊 Final DDQN Training Performance:\")\n",
    "print(env_name)\n",
    "print('Results: ', result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} ± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f} ± {result['episode_length_std']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# Save final model if agent has save_model method\n",
    "if hasattr(agent, 'save_model'):\n",
    "    agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_rewards_to_csv(reward_history, filename=\"rewards.csv\"):\n",
    "    with open(filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Episode\", \"Reward\"])\n",
    "        for i, reward in enumerate(reward_history):\n",
    "            writer.writerow([i, reward])\n",
    "\n",
    "def plot_rewards(reward_history, title=\"Reward Over Episodes\", save_path=\"DDQNreward_plot.png\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(reward_history, label=\"Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rewards_to_csv(reward_history, filename=\"DDQN_rewards.csv\")\n",
    "plot_rewards(reward_history, title=\"DDQN Training Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
