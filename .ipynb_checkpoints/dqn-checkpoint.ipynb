{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Deep Q-Network (DQN) </h2>\n",
    "<h4>Deep Q-Network (DQN) is a reinforcement learning algorithm that combines Q-learning with deep neural networks. \n",
    "    \n",
    "DQN replaces the Q-table with a neural network (the Q-network) that approximates the Q-function:\n",
    "Q(state, action) â‰ˆ predicted reward</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Setting up the environment and training for DQN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e src/gym-idsgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall gymnasium\n",
    "!pip install gym==0.21.0\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install gym-idsgame==1.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium.core import Env\n",
    "\n",
    "def patched_reset(self):\n",
    "    return self.reset()\n",
    "\n",
    "Env.reset = patched_reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE IMPORTING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import numpy as np\n",
    "import torch\n",
    "# src/environment/idsgame_wrapper.py\n",
    "# from src.agents.dqn_agent import DQNAgent\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.utils import print_summary\n",
    "from src.utils.plotting import plot_results\n",
    "# from src.utils import create_artefact_dirs\n",
    "\n",
    "print('DONE IMPORTING')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/Desktop/AI-Agent-for-Cyber-Security/missing_files_for_gym/*.py \\\n",
    "/usr/local/lib/python3.10/dist-packages/gym/utils/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from src.environment.explorer import IDSGameExplorer\n",
    "explorer = IDSGameExplorer()\n",
    "# explorer.run_comprehensive_exploration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Trainging and running the algorithm for \"idsgame-random_attack-v8\" environment</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishnaasrinivas/Desktop/AI-Agent-for-Cyber-Security/src/agents/dqn_agent.py\n"
     ]
    }
   ],
   "source": [
    "import src.agents.dqn_agent\n",
    "print(src.agents.dqn_agent.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# from gym_idsgame.agents.training_agents.q_learning.q_agent_config import QAgentConfig\n",
    "# from gym_idsgame.agents.training_agents.q_learning.dqn.dqn_config import DQNConfig\n",
    "from experiments.util import util\n",
    "from experiments.util.plotting_util import read_and_plot_results\n",
    "# from src.agents.ddqn_agent import DDQNAgent\n",
    "from src.utils.utils import get_output_dir, print_summary\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.plotting import plot_results\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment Information:\n",
      "Observation Space: Box(0, 9, (3, 11), int32)\n",
      "Action Space: Discrete(30)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"idsgame-random_defense-v0\"\n",
    "output_dir = os.getcwd()\n",
    "random_seed = 33\n",
    "env = gym.make(env_name, save_dir=output_dir + \"results/data/\" + str(random_seed))\n",
    "# env = GymCompatibilityWrapper(env)\n",
    "\n",
    "env = GymCompatibilityWrapper(env)\n",
    "env = env.unwrapped\n",
    "\n",
    "print(\"\\nEnvironment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_rewards_to_csv(reward_history, filename=\"rewards.csv\"):\n",
    "    with open(filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Episode\", \"Reward\"])\n",
    "        for i, reward in enumerate(reward_history):\n",
    "            writer.writerow([i, reward])\n",
    "\n",
    "def plot_rewards(reward_history, title=\"Reward Over Episodes\", save_path=\"reward_plot.png\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(reward_history, label=\"Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def print_summary(result, title=\"Summary\"):\n",
    "    \"\"\"\n",
    "    Print formatted training or evaluation summary.\n",
    "\n",
    "    Args:\n",
    "        result (dict): A dictionary containing metrics such as rewards, episode lengths, etc.\n",
    "        title (str): Title of the summary.\n",
    "    \"\"\"\n",
    "    print(f\"{title} Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Final Defense Performance:\")\n",
    "    print(f\"- Average Reward: {result['average_reward']:.2f} Â± {result['reward_std']:.2f}\")\n",
    "    print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "    print(f\"- Average Episode Length: {result['average_episode_length']:.2f} Â± {result['episode_length_std']:.2f}\")\n",
    "    print(f\"- Max-Min Episode Length: {result['max_episode_length']:.2f} - {result['min_episode_length']:.2f}\")\n",
    "    print(f\"- Average Hack Probability: {result['average_hack_probability']:.2f}% Â± {result['hack_probability_std']:.2f}%\")\n",
    "    print(f\"- Max-Min Hack Probability: {result['max_hack_probability']:.2f} - {result['min_hack_probability']:.2f}\")\n",
    "    print(f\"- Final Cumulative Reward: {int(result['cumulative_reward'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 0] AvgReward (last 10): -5.00 | Total: -5.00 | Epsilon: 1.000\n",
      "[Ep 1000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 2000] AvgReward (last 10): 0.40 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 3000] AvgReward (last 10): 0.80 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 4000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 5000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 6000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 7000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 8000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 9000] AvgReward (last 10): -4.90 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 10000] AvgReward (last 10): -8.80 | Total: -28.00 | Epsilon: 0.010\n",
      "[Ep 11000] AvgReward (last 10): -0.30 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 12000] AvgReward (last 10): -1.00 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 13000] AvgReward (last 10): -41.00 | Total: -26.00 | Epsilon: 0.010\n",
      "[Ep 14000] AvgReward (last 10): -1.00 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 15000] AvgReward (last 10): -18.60 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 16000] AvgReward (last 10): -16.30 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 17000] AvgReward (last 10): -0.80 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 18000] AvgReward (last 10): -44.80 | Total: -94.00 | Epsilon: 0.010\n",
      "[Ep 19000] AvgReward (last 10): -19.00 | Total: 1.00 | Epsilon: 0.010\n",
      "\n",
      "ðŸ“Š Final DQN Training Performance:\n",
      "Results:  {'average_reward': -12.2415, 'reward_std': 30.344958028476494, 'max_reward': 1, 'min_reward': -100, 'average_episode_length': 100, 'episode_length_std': 0.0, 'max_episode_length': 100, 'min_episode_length': 100, 'average_hack_probability': 0.0, 'hack_probability_std': 0.0, 'max_hack_probability': 0.0, 'min_hack_probability': 0.0, 'cumulative_reward': -244830}\n",
      "- Average Reward: -12.24 Â± 30.34\n",
      "- Max-Min Reward: 1.00 - -100.00\n",
      "- Average Episode Length: 100.00\n",
      "- Cumulative Reward: -244830\n"
     ]
    }
   ],
   "source": [
    "from src.agents.dqn_agent import DQNAgent\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_idsgame\n",
    "\n",
    "\n",
    "# === Get dimensions ===\n",
    "sample_obs = env.reset()[0]\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "# === Agent ===\n",
    "# agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    buffer_capacity=10000,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# === Training Loop ===\n",
    "reward_history = []\n",
    "num_episodes = 20000\n",
    "max_steps = 100\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state = extract_attacker_obs(env.reset())\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = agent.select_action(state)\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        full_action = (action, defense_action)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "\n",
    "        agent.store(state, action, reward[0], next_state, done)\n",
    "        agent.update()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward[0]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        avg_last_1000 = np.mean(reward_history[-10:])\n",
    "        print(f\"[Ep {ep}] AvgReward (last 10): {avg_last_1000:.2f} | Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "# === Evaluation Summary ===\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": max_steps,\n",
    "    \"episode_length_std\": 0.0,\n",
    "    \"max_episode_length\": max_steps,\n",
    "    \"min_episode_length\": max_steps,\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nðŸ“Š Final DQN Training Performance:\")\n",
    "print('Results: ',result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} Â± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 0] AvgReward (last 10): -20.00 | Total: -20.00 | Epsilon: 1.000\n",
      "[Ep 1000] AvgReward (last 10): -0.40 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 2000] AvgReward (last 10): 0.80 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 3000] AvgReward (last 10): -3.90 | Total: -1.00 | Epsilon: 0.010\n",
      "[Ep 4000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 5000] AvgReward (last 10): 0.90 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 6000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 7000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 8000] AvgReward (last 10): 0.80 | Total: 1.00 | Epsilon: 0.010\n",
      "[Ep 9000] AvgReward (last 10): 1.00 | Total: 1.00 | Epsilon: 0.010\n",
      "\n",
      "ðŸ“Š Final DQN Training Performance:\n",
      "Results:  {'average_reward': 0.4551, 'reward_std': 3.1199974342938175, 'max_reward': 1, 'min_reward': -96, 'average_episode_length': 100, 'episode_length_std': 0.0, 'max_episode_length': 100, 'min_episode_length': 100, 'average_hack_probability': 0.0, 'hack_probability_std': 0.0, 'max_hack_probability': 0.0, 'min_hack_probability': 0.0, 'cumulative_reward': 4551}\n",
      "- Average Reward: 0.46 Â± 3.12\n",
      "- Max-Min Reward: 1.00 - -96.00\n",
      "- Average Episode Length: 100.00\n",
      "- Cumulative Reward: 4551\n"
     ]
    }
   ],
   "source": [
    "from src.agents.dqn_agent import DQNAgent\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_idsgame\n",
    "\n",
    "\n",
    "# === Get dimensions ===\n",
    "sample_obs = env.reset()[0]\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "# === Agent ===\n",
    "# agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    buffer_capacity=10000,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# === Training Loop ===\n",
    "reward_history = []\n",
    "num_episodes = 10000\n",
    "max_steps = 100\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state = extract_attacker_obs(env.reset())\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = agent.select_action(state)\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        full_action = (action, defense_action)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "\n",
    "        agent.store(state, action, reward[0], next_state, done)\n",
    "        agent.update()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward[0]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        avg_last_1000 = np.mean(reward_history[-10:])\n",
    "        print(f\"[Ep {ep}] AvgReward (last 10): {avg_last_1000:.2f} | Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "# === Evaluation Summary ===\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": max_steps,\n",
    "    \"episode_length_std\": 0.0,\n",
    "    \"max_episode_length\": max_steps,\n",
    "    \"min_episode_length\": max_steps,\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nðŸ“Š Final DQN Training Performance:\")\n",
    "print('Results: ',result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} Â± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# # === Training ===\n",
    "# reward_history = []\n",
    "# num_episodes = 10000\n",
    "# max_steps = 100\n",
    "\n",
    "# def extract_attacker_obs(obs):\n",
    "#     obs = obs[0] if isinstance(obs, tuple) else obs\n",
    "#     return np.array(obs).flatten()\n",
    "\n",
    "# for ep in range(num_episodes):\n",
    "#     state = extract_attacker_obs(env.reset())\n",
    "#     total_reward = 0\n",
    "\n",
    "#     for t in range(max_steps):\n",
    "#         action = agent.select_action(state)\n",
    "#         def_action = env.defender_action_space.sample()\n",
    "#         full_action = (action, def_action)\n",
    "\n",
    "#         next_obs, reward, terminated, truncated, _ = env.step(full_action)\n",
    "#         done = terminated or truncated\n",
    "#         next_state = extract_attacker_obs(next_obs)\n",
    "\n",
    "#         agent.store(state, action, reward[0], next_state, done)\n",
    "#         agent.update()\n",
    "\n",
    "#         state = next_state\n",
    "#         total_reward += reward[0]\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     reward_history.append(total_reward)\n",
    "#     print(f'$$$$$$$$$$$$ ep: {ep}, reward : {total_reward}')\n",
    "\n",
    "#     # Logging\n",
    "#     if ep % 10 == 0:\n",
    "#         avg_last_10 = np.mean(reward_history[-10:])\n",
    "#         print(f\"[Ep {ep}] AvgReward (last 10): {avg_last_10:.2f} | Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rewards_to_csv(reward_history, filename=\"rewards.csv\")\n",
    "plot_rewards(reward_history, title=\"DQN Training Rewards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
