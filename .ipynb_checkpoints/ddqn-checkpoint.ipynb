{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# DDQN Agent for gym-idsgame (PyTorch Version)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to use the DDQN (Double Deep Q-Network) agent with the gym-idsgame environment. We'll cover:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Installation of required packages\\n\",\n",
    "    \"2. Setting up the environment\\n\",\n",
    "    \"3. Configuring the DDQN agent\\n\",\n",
    "    \"4. Training the agent\\n\",\n",
    "    \"5. Evaluating the agent\\n\",\n",
    "    \"6. Visualizing the results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Installation of Required Packages\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's install the necessary packages:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!pip install gym-idsgame==1.0.12\\n\",\n",
    "    \"!pip install torch\\n\",\n",
    "    \"!pip install matplotlib\\n\",\n",
    "    \"!pip install numpy\\n\",\n",
    "    \"!pip install gym\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Import DDQN Implementation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's import our DDQN implementation and other necessary libraries:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import gym\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import datetime\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import our DDQN implementation\\n\",\n",
    "    \"from DDQN import DDQNConfig, DDQNAgent, create_ddqn_agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if GPU is available\\n\",\n",
    "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Set up the Environment\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now, let's create the gym-idsgame environment:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Set random seed for reproducibility\\n\",\n",
    "    \"random_seed = 42\\n\",\n",
    "    \"np.random.seed(random_seed)\\n\",\n",
    "    \"torch.manual_seed(random_seed)\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    torch.cuda.manual_seed(random_seed)\\n\",\n",
    "    \"    torch.cuda.manual_seed_all(random_seed)\\n\",\n",
    "    \"    torch.backends.cudnn.deterministic = True\\n\",\n",
    "    \"    torch.backends.cudnn.benchmark = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create output directory for results\\n\",\n",
    "    \"current_time = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H-%M-%S\\\")\\n\",\n",
    "    \"result_dir = f\\\"./results/ddqn_pytorch_{current_time}\\\"\\n\",\n",
    "    \"if not os.path.exists(result_dir):\\n\",\n",
    "    \"    os.makedirs(result_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create environment\\n\",\n",
    "    \"env_name = \\\"idsgame-minimal_defense-v2\\\"\\n\",\n",
    "    \"env = gym.make(env_name, save_dir=result_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print environment information\\n\",\n",
    "    \"print(f\\\"Environment: {env_name}\\\")\\n\",\n",
    "    \"print(f\\\"Action space: {env.action_space}\\\")\\n\",\n",
    "    \"print(f\\\"Observation space: {env.observation_space}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Configure the DDQN Agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"Next, let's configure our DDQN agent with appropriate parameters:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Configure DDQN agent\\n\",\n",
    "    \"ddqn_config = DDQNConfig(\\n\",\n",
    "    \"    gamma=0.99,                       # Discount factor\\n\",\n",
    "    \"    lr=0.00005,                       # Learning rate\\n\",\n",
    "    \"    batch_size=64,                    # Batch size for training\\n\",\n",
    "    \"    epsilon=1.0,                      # Initial exploration rate\\n\",\n",
    "    \"    epsilon_decay=0.999,              # Decay rate for epsilon\\n\",\n",
    "    \"    min_epsilon=0.01,                 # Minimum value for epsilon\\n\",\n",
    "    \"    target_network_update_freq=100,   # Frequency of target network updates\\n\",\n",
    "    \"    replay_memory_size=10000,         # Size of replay buffer\\n\",\n",
    "    \"    num_episodes=5000,                # Number of episodes to train for\\n\",\n",
    "    \"    eval_frequency=500,               # Frequency of evaluations during training\\n\",\n",
    "    \"    eval_episodes=50,                 # Number of episodes for each evaluation\\n\",\n",
    "    \"    train_log_frequency=50,           # Frequency of logging during training\\n\",\n",
    "    \"    eval_log_frequency=1,             # Frequency of logging during evaluation\\n\",\n",
    "    \"    eval_render=False,                # Whether to render during evaluation\\n\",\n",
    "    \"    render=False,                     # Whether to render during training\\n\",\n",
    "    \"    attacker=True,                    # Whether the agent is an attacker\\n\",\n",
    "    \"    defender=False,                   # Whether the agent is a defender\\n\",\n",
    "    \"    save_dir=result_dir,              # Directory to save results\\n\",\n",
    "    \"    save_frequency=500,               # Frequency to save the model during training\\n\",\n",
    "    \"    hidden_layers=[128, 128],         # Hidden layer sizes for the neural network\\n\",\n",
    "    \"    device=device                     # Device to run the neural network on\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create DDQN agent\\n\",\n",
    "    \"agent = DDQNAgent(env, ddqn_config)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print agent information\\n\",\n",
    "    \"print(f\\\"Agent state dimension: {agent.state_dim}\\\")\\n\",\n",
    "    \"print(f\\\"Agent action dimension: {agent.num_actions}\\\")\\n\",\n",
    "    \"print(f\\\"Neural network hidden layers: {ddqn_config.hidden_layers}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Train the DDQN Agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now, let's train our DDQN agent:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train the agent\\n\",\n",
    "    \"print(\\\"Starting training...\\\")\\n\",\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train for specified number of episodes\\n\",\n",
    "    \"train_result = agent.train()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate training time\\n\",\n",
    "    \"training_time = time.time() - start_time\\n\",\n",
    "    \"print(f\\\"Training completed in {training_time:.2f} seconds\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Evaluate the Trained Agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now that we have trained our agent, let's evaluate its performance:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Final evaluation with more episodes\\n\",\n",
    "    \"print(\\\"Performing final evaluation...\\\")\\n\",\n",
    "    \"agent.config.eval_episodes = 100  # Increase number of evaluation episodes\\n\",\n",
    "    \"eval_result = agent.eval_model()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate average metrics\\n\",\n",
    "    \"avg_reward = sum(eval_result.episode_rewards) / len(eval_result.episode_rewards)\\n\",\n",
    "    \"avg_steps = sum(eval_result.episode_steps) / len(eval_result.episode_steps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Final Evaluation Results:\\\")\\n\",\n",
    "    \"print(f\\\"Average Reward: {avg_reward:.2f}\\\")\\n\",\n",
    "    \"print(f\\\"Average Steps: {avg_steps:.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Visualize the Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's visualize the training progress and the agent's performance:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training rewards\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.plot(agent.train_result.episode_rewards)\\n\",\n",
    "    \"plt.title('Episode Rewards')\\n\",\n",
    "    \"plt.xlabel('Episode')\\n\",\n",
    "    \"plt.ylabel('Reward')\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot running average of rewards\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"window_size = 100\\n\",\n",
    "    \"running_avg = [np.mean(agent.train_result.episode_rewards[max(0, i-window_size):i+1]) \\n\",\n",
    "    \"               for i in range(len(agent.train_result.episode_rewards))]\\n\",\n",
    "    \"plt.plot(running_avg)\\n\",\n",
    "    \"plt.title(f'Running Average Reward (Window Size: {window_size})')\\n\",\n",
    "    \"plt.xlabel('Episode')\\n\",\n",
    "    \"plt.ylabel('Average Reward')\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(os.path.join(result_dir, 'training_rewards.png'))\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot action distribution during training\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get action counts\\n\",\n",
    "    \"actions = list(agent.train_result.action_counts.keys())\\n\",\n",
    "    \"counts = list(agent.train_result.action_counts.values())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort by action index\\n\",\n",
    "    \"action_counts = sorted(zip(actions, counts))\\n\",\n",
    "    \"actions, counts = zip(*action_counts)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot action distribution\\n\",\n",
    "    \"plt.bar(actions, counts)\\n\",\n",
    "    \"plt.title('Action Distribution During Training')\\n\",\n",
    "    \"plt.xlabel('Action')\\n\",\n",
    "    \"plt.ylabel('Count')\\n\",\n",
    "    \"plt.grid(True, axis='y')\\n\",\n",
    "    \"plt.savefig(os.path.join(result_dir, 'action_distribution.png'))\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Save and Load the Trained Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's save our trained model for future use and demonstrate how to load it:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the trained model\\n\",\n",
    "    \"save_path = os.path.join(result_dir, \\\"final_model\\\")\\n\",\n",
    "    \"if not os.path.exists(save_path):\\n\",\n",
    "    \"    os.makedirs(save_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save policy network\\n\",\n",
    "    \"torch.save(agent.policy_network.state_dict(), os.path.join(save_path, \\\"policy_network.pt\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save training metrics\\n\",\n",
    "    \"np.save(os.path.join(save_path, \\\"training_metrics.npy\\\"), {\\n\",\n",
    "    \"    \\\"episode_rewards\\\": agent.train_result.episode_rewards,\\n\",\n",
    "    \"    \\\"avg_episode_rewards\\\": agent.avg_episode_rewards,\\n\",\n",
    "    \"    \\\"action_counts\\\": agent.train_result.action_counts\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model saved to: {save_path}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example of loading the model\\n\",\n",
    "    \"def load_model(model_path, env):\\n\",\n",
    "    \"    # Create a new agent\\n\",\n",
    "    \"    config = DDQNConfig(\\n\",\n",
    "    \"        attacker=True,\\n\",\n",
    "    \"        defender=False,\\n\",\n",
    "    \"        save_dir=result_dir,\\n\",\n",
    "    \"        hidden_layers=[128, 128],\\n\",\n",
    "    \"        device=device\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    loaded_agent = DDQNAgent(env, config)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load model weights\\n\",\n",
    "    \"    policy_path = os.path.join(model_path, \\\"policy_network.pt\\\")\\n\",\n",
    "    \"    loaded_agent.policy_network.load_state_dict(torch.load(policy_path, map_location=device))\\n\",\n",
    "    \"    loaded_agent.target_network.load_state_dict(torch.load(policy_path, map_location=device))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Set to evaluation mode\\n\",\n",
    "    \"    loaded_agent.policy_network.eval()\\n\",\n",
    "    \"    loaded_agent.target_network.eval()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return loaded_agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Uncomment to load the model\\n\",\n",
    "    \"# loaded_agent = load_model(save_path, env)\\n\",\n",
    "    \"# test_result = loaded_agent.eval_model()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Running a Demo with the Trained Agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's run a demo with our trained agent and see how it performs in the environment:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def run_demo(agent, episodes=5, render=True, sleep_time=0.1):\\n\",\n",
    "    \"    \\\"\\\"\\\"Run a demo of the trained agent\\\"\\\"\\\"\\n\",\n",
    "    \"    print(\\\"\\\\nRunning demo with trained agent...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for episode in range(episodes):\\n\",\n",
    "    \"        state = env.reset()\\n\",\n",
    "    \"        if isinstance(state, tuple):\\n\",\n",
    "    \"            state = state[0]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        episode_reward = 0\\n\",\n",
    "    \"        episode_steps = 0\\n\",\n",
    "    \"        done = False\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nEpisode {episode+1}:\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        while not done:\\n\",\n",
    "    \"            if render:\\n\",\n",
    "    \"                env.render()\\n\",\n",
    "    \"            if sleep_time > 0:\\n\",\n",
    "    \"                time.sleep(sleep_time)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Select best action\\n\",\n",
    "    \"            state_tensor = torch.FloatTensor(state).to(device)\\n\",\n",
    "    \"            with torch.no_grad():\\n\",\n",
    "    \"                q_values = agent.policy_network(state_tensor)\\n\",\n",
    "    \"            action = q_values.argmax().item()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            next_state, reward, done, _ = env.step(action)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"Step {episode_steps}: Action={action}, Reward={reward}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            state = next_state\\n\",\n",
    "    \"            episode_reward += reward\\n\",\n",
    "    \"            episode_steps += 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if hasattr(env, 'idsgame_config') and episode_steps >= env.idsgame_config.game_config.max_steps:\\n\",\n",
    "    \"                done = True\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"Episode {episode+1} finished with total reward: {episode_reward}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if render:\\n\",\n",
    "    \"        env.close()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Uncomment to run the demo with the trained agent\\n\",\n",
    "    \"# run_demo(agent, episodes=3, render=True, sleep_time=0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Conclusion\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we have:\\n\",\n",
    "    \"1. Implemented a DDQN agent using PyTorch\\n\",\n",
    "    \"2. Configured and trained the agent in the gym-idsgame environment\\n\",\n",
    "    \"3. Evaluated the agent's performance\\n\",\n",
    "    \"4. Visualized the training results\\n\",\n",
    "    \"5. Demonstrated how to save and load the trained model\\n\",\n",
    "    \"6. Created a demo to showcase the trained agent's behavior\\n\",\n",
    "    \"\\n\",\n",
    "    \"The DDQN agent learns to make strategic decisions in the intrusion detection game by balancing exploration and exploitation through an epsilon-greedy policy, and using experience replay to learn from past experiences. The Double DQN approach helps to prevent overestimation of Q-values, leading to more stable and effective learning.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e src/gym-idsgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping gymnasium as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gym==0.21.0\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'tests_require'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting gym-idsgame==1.0.12\n",
      "  Using cached gym_idsgame-1.0.12-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting gym (from gym-idsgame==1.0.12)\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Collecting pyglet (from gym-idsgame==1.0.12)\n",
      "  Using cached pyglet-2.1.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/gym-idsgame/lib/python3.13/site-packages (from gym-idsgame==1.0.12) (2.2.4)\n",
      "Collecting torch (from gym-idsgame==1.0.12)\n",
      "  Using cached torch-2.6.0-cp313-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting matplotlib (from gym-idsgame==1.0.12)\n",
      "  Using cached matplotlib-3.10.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from gym-idsgame==1.0.12)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting tqdm (from gym-idsgame==1.0.12)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting opencv-python (from gym-idsgame==1.0.12)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting imageio (from gym-idsgame==1.0.12)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jsonpickle (from gym-idsgame==1.0.12)\n",
      "  Using cached jsonpickle-4.0.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting tensorboard (from gym-idsgame==1.0.12)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sklearn (from gym-idsgame==1.0.12)\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip uninstall gymnasium\n",
    "!pip install gym==0.21.0\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install gym-idsgame==1.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium.core import Env\n",
    "\n",
    "def patched_reset(self):\n",
    "    return self.reset()\n",
    "\n",
    "Env.reset = patched_reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE IMPORTING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import numpy as np\n",
    "import torch\n",
    "# src/environment/idsgame_wrapper.py\n",
    "# from src.agents.dqn_agent import DQNAgent\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.utils import print_summary\n",
    "from src.utils.plotting import plot_results\n",
    "# from src.utils import create_artefact_dirs\n",
    "\n",
    "print('DONE IMPORTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from src.environment.explorer import IDSGameExplorer\n",
    "explorer = IDSGameExplorer()\n",
    "# explorer.run_comprehensive_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishnaasrinivas/Desktop/AI-Agent-for-Cyber-Security/src/agents/ddqn_agent.py\n"
     ]
    }
   ],
   "source": [
    "import src.agents.ddqn_agent\n",
    "print(src.agents.ddqn_agent.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# from gym_idsgame.agents.training_agents.q_learning.q_agent_config import QAgentConfig\n",
    "# from gym_idsgame.agents.training_agents.q_learning.dqn.dqn_config import DQNConfig\n",
    "from experiments.util import util\n",
    "from experiments.util.plotting_util import read_and_plot_results\n",
    "# from src.agents.ddqn_agent import DDQNAgent\n",
    "from src.utils.utils import get_output_dir, print_summary\n",
    "from src.environment.compatibility_wrapper import GymCompatibilityWrapper\n",
    "from src.utils.plotting import plot_results\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment Information:\n",
      "Observation Space: Box(0, 9, (1, 11), int32)\n",
      "Action Space: Discrete(30)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"idsgame-random_attack-v8\"\n",
    "output_dir = os.getcwd()\n",
    "random_seed = 42\n",
    "env = gym.make(env_name, save_dir=output_dir + \"results/data/\" + str(random_seed))\n",
    "# env = GymCompatibilityWrapper(env)\n",
    "\n",
    "env = GymCompatibilityWrapper(env)\n",
    "env = env.unwrapped\n",
    "\n",
    "print(\"\\nEnvironment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: idsgame-random_attack-v8\n",
      "Action space: Discrete(30)\n",
      "Observation space: Box(0, 9, (1, 11), int32)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create output directory for results\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "result_dir = f\"./results/ddqn_pytorch_{current_time}\"\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "# # Create environment\n",
    "# env_name = \"idsgame-random_defense-v0\"\n",
    "# env = gym.make(env_name, save_dir=result_dir)\n",
    "\n",
    "# Print environment information\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state dimension: 30\n",
      "Agent action dimension: 30\n",
      "Neural network hidden layers: [128, 128]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "\n",
    "# Configure DDQN agent\\n\",\n",
    "ddqn_config = DDQNConfig(\n",
    "       gamma=0.99,                       # Discount factor\\n\",\n",
    "        lr=0.00005,                       # Learning rate\\n\",\n",
    "        batch_size=64,                    # Batch size for training\\n\",\n",
    "      epsilon=1.0,                      # Initial exploration rate\\n\",\n",
    "       epsilon_decay=0.999,              # Decay rate for epsilon\\n\",\n",
    " min_epsilon=0.01,                 # Minimum value for epsilon\\n\",\n",
    "   target_network_update_freq=100,   # Frequency of target network updates\\n\",\n",
    "    replay_memory_size=10000,         # Size of replay buffer\\n\",\n",
    "   num_episodes=5000,                # Number of episodes to train for\\n\",\n",
    "  eval_frequency=500,               # Frequency of evaluations during training\\n\",\n",
    "  eval_episodes=50,                 # Number of episodes for each evaluation\\n\",\n",
    "  train_log_frequency=50,           # Frequency of logging during training\\n\",\n",
    "  eval_log_frequency=1,             # Frequency of logging during evaluation\\n\",\n",
    " eval_render=False,                # Whether to render during evaluation\\n\",\n",
    " render=False,                     # Whether to render during training\\n\",\n",
    " attacker=True,                    # Whether the agent is an attacker\\n\",\n",
    "defender=False,                   # Whether the agent is a defender\\n\",\n",
    " save_dir=result_dir,              # Directory to save results\\n\",\n",
    " save_frequency=500,               # Frequency to save the model during training\\n\",\n",
    " hidden_layers=[128, 128],         # Hidden layer sizes for the neural network\\n\",\n",
    " device=device                     # Device to run the neural network on\\n\",\n",
    "    )\n",
    "agent = DDQNAgent(env, ddqn_config)\n",
    "\n",
    "# Print agent information\n",
    "print(f\"Agent state dimension: {agent.state_dim}\")\n",
    "print(f\"Agent action dimension: {agent.num_actions}\")\n",
    "print(f\"Neural network hidden layers: {ddqn_config.hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(result, title=\"\"):\n",
    "    print(f\"\\n{title} Summary:\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"- Average Reward: {np.mean(result.episode_rewards):.2f} ± {np.std(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Max Reward: {np.max(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Min Reward: {np.min(result.episode_rewards):.2f}\")\n",
    "    print(f\"- Avg Episode Length: {np.mean(result.episode_steps):.2f} steps\")\n",
    "\n",
    "    # Call this after training\n",
    "    print_summary(agent.train_result, \"Training\")\n",
    "    print_summary(agent.eval_result, \"Evaluation\")\n",
    "\n",
    "    print(f\"- Cumulative Reward: {sum(result.episode_rewards)}\")\n",
    "\n",
    "def plot_training(agent):\n",
    "    \"\"\"Plot training progress\"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Smoothed rewards\n",
    "    rewards = agent.train_result.episode_rewards\n",
    "    window_size = 50\n",
    "    smooth_rewards = [np.mean(rewards[max(0,i-window_size):i+1]) \n",
    "                     for i in range(len(rewards))]\n",
    "    \n",
    "    plt.plot(smooth_rewards)\n",
    "    plt.title(\"Training Progress (Smoothed Rewards)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: DDQNConfig.__init__() got an unexpected keyword argument 'tau'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDQNAgent(env, config)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 2. Train Agent\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_agent(agent, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \u001b[38;5;66;03m# Modified training loop\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3. Final Evaluation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m final_eval_result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39meval_model()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_agent' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = DDQNConfig(\n",
    "        batch_size=64,\n",
    "        lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        tau =0.005,  # Test soft update parameter\n",
    "        clip_norm=10.0  # Test gradient clipping parameter\n",
    "    )\n",
    "    print(\"DDQNConfig initialized successfully with tau and clip_norm.\")\n",
    "except TypeError as e:\n",
    "    print(f\"TypeError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "agent = DDQNAgent(env, config)\n",
    "\n",
    "# 2. Train Agent\n",
    "train_agent(agent, episodes=1000)  # Modified training loop\n",
    "\n",
    "# 3. Final Evaluation\n",
    "final_eval_result = agent.eval_model()\n",
    "print_summary(final_eval_result, \"Final Evaluation\")\n",
    "\n",
    "# 4. Save Results\n",
    "agent.save_model()\n",
    "plot_training(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Environment: idsgame-random_attack-v8\n",
      "State dimension: 33\n",
      "Action dimension: 30\n",
      "Starting training for 20000 episodes...\n",
      "[Ep 1/20000] AvgReward (last 1): -1.00 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 0.0s\n",
      "[Ep 500/20000] AvgReward (last 500): -0.10 | Total: 0.17 | Epsilon: 1.000 | Steps: 9 | Time: 0.6s\n",
      "[Ep 1000/20000] AvgReward (last 500): -0.06 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 1.2s\n",
      "[Ep 1500/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 1.8s\n",
      "[Ep 2000/20000] AvgReward (last 500): -0.12 | Total: -1.00 | Epsilon: 1.000 | Steps: 6 | Time: 2.4s\n",
      "[Ep 2500/20000] AvgReward (last 500): -0.08 | Total: 1.00 | Epsilon: 1.000 | Steps: 3 | Time: 3.0s\n",
      "[Ep 3000/20000] AvgReward (last 500): -0.00 | Total: -1.00 | Epsilon: 1.000 | Steps: 8 | Time: 3.9s\n",
      "[Ep 3500/20000] AvgReward (last 500): -0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 9 | Time: 4.5s\n",
      "[Ep 4000/20000] AvgReward (last 500): 0.03 | Total: 1.00 | Epsilon: 1.000 | Steps: 7 | Time: 5.3s\n",
      "[Ep 4500/20000] AvgReward (last 500): -0.09 | Total: 0.33 | Epsilon: 1.000 | Steps: 5 | Time: 6.1s\n",
      "[Ep 5000/20000] AvgReward (last 500): 0.01 | Total: 3.50 | Epsilon: 1.000 | Steps: 4 | Time: 6.7s\n",
      "[Ep 5500/20000] AvgReward (last 500): -0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 11 | Time: 7.3s\n",
      "[Ep 6000/20000] AvgReward (last 500): -0.05 | Total: 1.00 | Epsilon: 1.000 | Steps: 5 | Time: 8.0s\n",
      "[Ep 6500/20000] AvgReward (last 500): -0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 8.6s\n",
      "[Ep 7000/20000] AvgReward (last 500): -0.06 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 9.2s\n",
      "[Ep 7500/20000] AvgReward (last 500): -0.21 | Total: 1.00 | Epsilon: 1.000 | Steps: 2 | Time: 9.7s\n",
      "[Ep 8000/20000] AvgReward (last 500): 0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 10.3s\n",
      "[Ep 8500/20000] AvgReward (last 500): -0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 10.9s\n",
      "[Ep 9000/20000] AvgReward (last 500): -0.05 | Total: 2.67 | Epsilon: 1.000 | Steps: 8 | Time: 11.5s\n",
      "[Ep 9500/20000] AvgReward (last 500): -0.08 | Total: 1.00 | Epsilon: 1.000 | Steps: 9 | Time: 12.0s\n",
      "[Ep 10000/20000] AvgReward (last 500): -0.02 | Total: 2.67 | Epsilon: 1.000 | Steps: 10 | Time: 12.6s\n",
      "[Ep 10500/20000] AvgReward (last 500): -0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 11 | Time: 13.2s\n",
      "[Ep 11000/20000] AvgReward (last 500): -0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 13.8s\n",
      "[Ep 11500/20000] AvgReward (last 500): 0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 6 | Time: 14.4s\n",
      "[Ep 12000/20000] AvgReward (last 500): -0.01 | Total: -1.00 | Epsilon: 1.000 | Steps: 5 | Time: 15.0s\n",
      "[Ep 12500/20000] AvgReward (last 500): -0.03 | Total: 2.33 | Epsilon: 1.000 | Steps: 15 | Time: 15.6s\n",
      "[Ep 13000/20000] AvgReward (last 500): -0.06 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 16.1s\n",
      "[Ep 13500/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 16.7s\n",
      "[Ep 14000/20000] AvgReward (last 500): -0.10 | Total: 4.00 | Epsilon: 1.000 | Steps: 3 | Time: 17.2s\n",
      "[Ep 14500/20000] AvgReward (last 500): -0.04 | Total: -1.00 | Epsilon: 1.000 | Steps: 1 | Time: 17.9s\n",
      "[Ep 15000/20000] AvgReward (last 500): -0.10 | Total: -1.00 | Epsilon: 1.000 | Steps: 2 | Time: 18.4s\n",
      "[Ep 15500/20000] AvgReward (last 500): -0.14 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 19.0s\n",
      "[Ep 16000/20000] AvgReward (last 500): -0.08 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 19.6s\n",
      "[Ep 16500/20000] AvgReward (last 500): -0.00 | Total: 1.00 | Epsilon: 1.000 | Steps: 5 | Time: 20.1s\n",
      "[Ep 17000/20000] AvgReward (last 500): 0.02 | Total: 2.60 | Epsilon: 1.000 | Steps: 14 | Time: 20.7s\n",
      "[Ep 17500/20000] AvgReward (last 500): 0.04 | Total: 1.00 | Epsilon: 1.000 | Steps: 6 | Time: 21.3s\n",
      "[Ep 18000/20000] AvgReward (last 500): -0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 4 | Time: 21.9s\n",
      "[Ep 18500/20000] AvgReward (last 500): -0.05 | Total: -1.00 | Epsilon: 1.000 | Steps: 3 | Time: 22.5s\n",
      "[Ep 19000/20000] AvgReward (last 500): -0.01 | Total: 0.50 | Epsilon: 1.000 | Steps: 4 | Time: 23.0s\n",
      "[Ep 19500/20000] AvgReward (last 500): -0.02 | Total: -1.00 | Epsilon: 1.000 | Steps: 10 | Time: 23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n",
      "2025-04-11 01:04:34,713 - DDQN_Agent - INFO - Model saved to: ./results/ddqn_2025-04-11_01-04-10/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 20000/20000] AvgReward (last 500): -0.08 | Total: 1.00 | Epsilon: 1.000 | Steps: 2 | Time: 24.2s\n",
      "\n",
      "Training completed in 24.2 seconds\n",
      "\n",
      "📊 Final DDQN Training Performance:\n",
      "idsgame-random_attack-v8\n",
      "Results:  {'average_reward': -0.04738668110484288, 'reward_std': 1.4632312443832924, 'max_reward': 4.0, 'min_reward': -1.0, 'average_episode_length': 4.6359, 'episode_length_std': 3.5468621611221374, 'max_episode_length': 27, 'min_episode_length': 1, 'average_hack_probability': 0.0, 'hack_probability_std': 0.0, 'max_hack_probability': 0.0, 'min_hack_probability': 0.0, 'cumulative_reward': -947}\n",
      "- Average Reward: -0.05 ± 1.46\n",
      "- Max-Min Reward: 4.00 - -1.00\n",
      "- Average Episode Length: 4.64 ± 3.55\n",
      "- Cumulative Reward: -947\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from src.agents.ddqn_agent import DDQNConfig, DDQNAgent, create_ddqn_agent\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def extract_attacker_obs(obs):\n",
    "    \"\"\"Extract and flatten attacker observation\"\"\"\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Handle gym>=0.26.0 reset() returning (obs, info)\n",
    "    return np.array(obs).flatten()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # Create environment\n",
    "# env_name = \"idsgame-minimal_defense-v2\"\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "result_dir = f\"./results/ddqn_{timestamp}\"\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "sample_obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "\n",
    "# Create DDQN configuration\n",
    "config = DDQNConfig(\n",
    "    gamma=0.99,                       # Discount factor\n",
    "    lr=1e-3,                          # Learning rate\n",
    "    batch_size=32,                    # Reduce batch size to improve stability\n",
    "    epsilon=1.0,                      # Initial exploration rate\n",
    "    epsilon_decay=0.995,              # Decay rate for epsilon\n",
    "    min_epsilon=0.01,                 # Minimum value for epsilon\n",
    "    target_network_update_freq=10,    # Update target network frequency\n",
    "    replay_memory_size=10000,         # Size of replay buffer\n",
    "    num_episodes=10000,               # Number of episodes to train for\n",
    "    train_log_frequency=100,          # Log progress every 100 episodes\n",
    "    eval_frequency=1000,              # Evaluate every 1000 episodes\n",
    "    save_dir=result_dir,              # Directory to save results\n",
    "    hidden_layers=[64, 64]            # Smaller hidden layers for stability\n",
    ")\n",
    "agent = DDQNAgent(env, config)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 20000\n",
    "max_steps = 100\n",
    "log_frequency = 500\n",
    "\n",
    "# For tracking progress\n",
    "reward_history = []\n",
    "episode_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for ep in range(num_episodes):\n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]\n",
    "        \n",
    "    state = extract_attacker_obs(obs)  # Make sure state is properly formatted\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_steps):\n",
    "        # Select attacker action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Get defender action (random)\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        \n",
    "        # Combine actions for the environment\n",
    "        full_action = (action, defense_action)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_obs, reward, terminated, truncated, info = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Get attacker reward\n",
    "        attacker_reward = reward[0] if isinstance(reward, tuple) else reward\n",
    "        \n",
    "        # Extract and format next state\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.replay_buffer.push(state, action, attacker_reward, next_state, bool(done))\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += attacker_reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Store episode metrics\n",
    "    reward_history.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Log progress\n",
    "    if (ep + 1) % log_frequency == 0 or ep == 0:\n",
    "        avg_reward = np.mean(reward_history[-min(log_frequency, len(reward_history)):]) \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Ep {ep+1}/{num_episodes}] AvgReward (last {min(log_frequency, len(reward_history))}): {avg_reward:.2f} | \"\n",
    "              f\"Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | \"\n",
    "              f\"Steps: {steps} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "# Agent training should have been updating automatically through the \n",
    "# agent's internal train() method, if implemented properly\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": np.mean(episode_lengths),\n",
    "    \"episode_length_std\": np.std(episode_lengths),\n",
    "    \"max_episode_length\": np.max(episode_lengths),\n",
    "    \"min_episode_length\": np.min(episode_lengths),\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"\\n📊 Final DDQN Training Performance:\")\n",
    "print(env_name)\n",
    "print('Results: ', result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} ± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f} ± {result['episode_length_std']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# Save final model if agent has save_model method\n",
    "if hasattr(agent, 'save_model'):\n",
    "    agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: idsgame-random_defense-v0\n",
      "State dimension: 33\n",
      "Action dimension: 30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DDQNAgent.__init__() got an unexpected keyword argument 'state_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDQNAgent(\n\u001b[1;32m     10\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39mstate_dim,\n\u001b[1;32m     11\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39maction_dim,\n\u001b[1;32m     12\u001b[0m     buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,     \u001b[38;5;66;03m# Replay buffer size\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,                \u001b[38;5;66;03m# Discount factor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,                   \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,             \u001b[38;5;66;03m# Training batch size\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     epsilon_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,         \u001b[38;5;66;03m# Initial exploration rate\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     epsilon_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,          \u001b[38;5;66;03m# Final exploration rate\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.995\u001b[39m,       \u001b[38;5;66;03m# Exploration decay rate\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     update_target_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,    \u001b[38;5;66;03m# Target network update frequency\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[1;32m     24\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: DDQNAgent.__init__() got an unexpected keyword argument 'state_dim'"
     ]
    }
   ],
   "source": [
    "sample_obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "agent = DDQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    buffer_capacity=10000,     # Replay buffer size\n",
    "    gamma=0.99,                # Discount factor\n",
    "    lr=1e-3,                   # Learning rate\n",
    "    batch_size=64,             # Training batch size\n",
    "    epsilon_start=1.0,         # Initial exploration rate\n",
    "    epsilon_end=0.01,          # Final exploration rate\n",
    "    epsilon_decay=0.995,       # Exploration decay rate\n",
    "    update_target_every=10,    # Target network update frequency\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "max_steps = 100\n",
    "log_frequency = 500\n",
    "\n",
    "# For tracking progress\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "episode_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for ep in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = extract_attacker_obs(env.reset())\n",
    "    total_reward = 0\n",
    "    episode_loss = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_steps):\n",
    "        # Select attacker action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Select random defender action\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        \n",
    "        # Combine actions and take step\n",
    "        full_action = (action, defense_action)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Extract attacker reward\n",
    "        attacker_reward = reward[0] if isinstance(reward, tuple) else reward\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.store(state, action, attacker_reward, next_state, done)\n",
    "        \n",
    "        # Update agent\n",
    "        loss = agent.update()\n",
    "        if loss:\n",
    "            episode_loss += loss\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += attacker_reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Store episode metrics\n",
    "    reward_history.append(total_reward)\n",
    "    loss_history.append(episode_loss / max(1, steps))\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Log progress\n",
    "    if (ep + 1) % log_frequency == 0 or ep == 0:\n",
    "        avg_reward = np.mean(reward_history[-min(log_frequency, len(reward_history)):]) \n",
    "        avg_loss = np.mean(loss_history[-min(log_frequency, len(loss_history)):])/max(1, min(log_frequency, len(loss_history)))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Ep {ep+1}/{num_episodes}] AvgReward (last {min(log_frequency, len(reward_history))}): {avg_reward:.2f} | \"\n",
    "              f\"Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | Loss: {avg_loss:.4f} | \"\n",
    "              f\"Steps: {steps} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": np.mean(episode_lengths),\n",
    "    \"episode_length_std\": np.std(episode_lengths),\n",
    "    \"max_episode_length\": np.max(episode_lengths),\n",
    "    \"min_episode_length\": np.min(episode_lengths),\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"\\n📊 Final DDQN Training Performance:\")\n",
    "print('Results: ', result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} ± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f} ± {result['episode_length_std']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# Save the trained agent\n",
    "save_path = \"./ddqn_model.pt\"\n",
    "agent.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Environment: idsgame-random_defense-v0\n",
      "State dimension: 33\n",
      "Action dimension: 30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DDQNAgent.__init__() got an unexpected keyword argument 'expected_state_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Create DDQN agent - adjusting to match your implementation\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDQNAgent(\n\u001b[1;32m     31\u001b[0m     expected_state_dim\u001b[38;5;241m=\u001b[39mstate_dim,  \u001b[38;5;66;03m# Assuming this is the parameter name in your code\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39maction_dim,\n\u001b[1;32m     33\u001b[0m     buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,     \u001b[38;5;66;03m# Replay buffer size\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,                \u001b[38;5;66;03m# Discount factor\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,                   \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,             \u001b[38;5;66;03m# Training batch size\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     epsilon_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,         \u001b[38;5;66;03m# Initial exploration rate\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     epsilon_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,          \u001b[38;5;66;03m# Final exploration rate\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.995\u001b[39m,       \u001b[38;5;66;03m# Exploration decay rate\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[1;32m     44\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: DDQNAgent.__init__() got an unexpected keyword argument 'expected_state_dim'"
     ]
    }
   ],
   "source": [
    "# Import DDQN agent and necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import time\n",
    "# from DDQN import DDQNAgent, extract_attacker_obs, set_random_seeds\n",
    "\n",
    "# Set random seed\n",
    "random_seeds = 42\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # Create environment\n",
    "# env_name = \"idsgame-minimal_defense-v2\"\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# Get dimensions\n",
    "sample_obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
    "state_dim = np.array(sample_obs).flatten().shape[0]\n",
    "action_dim = env.attacker_action_space.n\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Create DDQN agent - adjusting to match your implementation\n",
    "agent = DDQNAgent(\n",
    "    expected_state_dim=state_dim,  # Assuming this is the parameter name in your code\n",
    "    action_dim=action_dim,\n",
    "    buffer_capacity=10000,     # Replay buffer size\n",
    "    gamma=0.99,                # Discount factor\n",
    "    lr=1e-3,                   # Learning rate\n",
    "    batch_size=64,             # Training batch size\n",
    "    epsilon_start=1.0,         # Initial exploration rate\n",
    "    epsilon_end=0.01,          # Final exploration rate\n",
    "    epsilon_decay=0.995,       # Exploration decay rate\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "max_steps = 100\n",
    "log_frequency = 500\n",
    "\n",
    "# For tracking progress\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "episode_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for ep in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = extract_attacker_obs(env.reset())\n",
    "    total_reward = 0\n",
    "    episode_loss = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_steps):\n",
    "        # Select attacker action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Select random defender action\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        \n",
    "        # Combine actions and take step\n",
    "        full_action = (action, defense_action)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Extract attacker reward\n",
    "        attacker_reward = reward[0] if isinstance(reward, tuple) else reward\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.store(state, action, attacker_reward, next_state, done)\n",
    "        \n",
    "        # Update agent\n",
    "        loss = agent.update()\n",
    "        if loss:\n",
    "            episode_loss += loss\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += attacker_reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Store episode metrics\n",
    "    reward_history.append(total_reward)\n",
    "    loss_history.append(episode_loss / max(1, steps))\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Log progress\n",
    "    if (ep + 1) % log_frequency == 0 or ep == 0:\n",
    "        avg_reward = np.mean(reward_history[-min(log_frequency, len(reward_history)):]) \n",
    "        avg_loss = np.mean(loss_history[-min(log_frequency, len(loss_history)):])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Ep {ep+1}/{num_episodes}] AvgReward (last {min(log_frequency, len(reward_history))}): {avg_reward:.2f} | \"\n",
    "              f\"Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | Loss: {avg_loss:.4f} | \"\n",
    "              f\"Steps: {steps} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": np.mean(episode_lengths),\n",
    "    \"episode_length_std\": np.std(episode_lengths),\n",
    "    \"max_episode_length\": np.max(episode_lengths),\n",
    "    \"min_episode_length\": np.min(episode_lengths),\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"\\n📊 Final DDQN Training Performance:\")\n",
    "print('Results: ', result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} ± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f} ± {result['episode_length_std']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# Save the trained agent if it has a save method\n",
    "if hasattr(agent, 'save'):\n",
    "    save_path = \"./ddqn_model.pt\"\n",
    "    agent.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1082262925.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[36], line 97\u001b[0;36m\u001b[0m\n\u001b[0;31m    agent.save(save_path)RetryClaude can make mistakes. Please double-check responses.\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "max_steps = 100\n",
    "log_frequency = 500\n",
    "\n",
    "# For tracking progress\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "episode_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "for ep in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = extract_attacker_obs(env.reset())\n",
    "    total_reward = 0\n",
    "    episode_loss = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    for t in range(max_steps):\n",
    "        # Select attacker action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Select random defender action\n",
    "        defense_action = env.defender_action_space.sample()\n",
    "        \n",
    "        # Combine actions and take step\n",
    "        full_action = (action, defense_action)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(full_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Extract attacker reward\n",
    "        attacker_reward = reward[0] if isinstance(reward, tuple) else reward\n",
    "        next_state = extract_attacker_obs(next_obs)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.store(state, action, attacker_reward, next_state, done)\n",
    "        \n",
    "        # Update agent\n",
    "        loss = agent.update()\n",
    "        if loss:\n",
    "            episode_loss += loss\n",
    "        \n",
    "        # Update state and metrics\n",
    "        state = next_state\n",
    "        total_reward += attacker_reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Store episode metrics\n",
    "    reward_history.append(total_reward)\n",
    "    loss_history.append(episode_loss / max(1, steps))\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Log progress\n",
    "    if (ep + 1) % log_frequency == 0 or ep == 0:\n",
    "        avg_reward = np.mean(reward_history[-min(log_frequency, len(reward_history)):]) \n",
    "        avg_loss = np.mean(loss_history[-min(log_frequency, len(loss_history)):])/max(1, min(log_frequency, len(loss_history)))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Ep {ep+1}/{num_episodes}] AvgReward (last {min(log_frequency, len(reward_history))}): {avg_reward:.2f} | \"\n",
    "              f\"Total: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | Loss: {avg_loss:.4f} | \"\n",
    "              f\"Steps: {steps} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "result = {\n",
    "    \"average_reward\": np.mean(reward_history),\n",
    "    \"reward_std\": np.std(reward_history),\n",
    "    \"max_reward\": np.max(reward_history),\n",
    "    \"min_reward\": np.min(reward_history),\n",
    "    \"average_episode_length\": np.mean(episode_lengths),\n",
    "    \"episode_length_std\": np.std(episode_lengths),\n",
    "    \"max_episode_length\": np.max(episode_lengths),\n",
    "    \"min_episode_length\": np.min(episode_lengths),\n",
    "    \"average_hack_probability\": 0.0,\n",
    "    \"hack_probability_std\": 0.0,\n",
    "    \"max_hack_probability\": 0.0,\n",
    "    \"min_hack_probability\": 0.0,\n",
    "    \"cumulative_reward\": int(np.sum(reward_history)),\n",
    "}\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"\\n📊 Final DDQN Training Performance:\")\n",
    "print('Results: ', result)\n",
    "print(f\"- Average Reward: {result['average_reward']:.2f} ± {result['reward_std']:.2f}\")\n",
    "print(f\"- Max-Min Reward: {result['max_reward']:.2f} - {result['min_reward']:.2f}\")\n",
    "print(f\"- Average Episode Length: {result['average_episode_length']:.2f} ± {result['episode_length_std']:.2f}\")\n",
    "print(f\"- Cumulative Reward: {result['cumulative_reward']}\")\n",
    "\n",
    "# Save the trained agent\n",
    "save_path = \"./ddqn_model.pt\"\n",
    "agent.save(save_path)RetryClaude can make mistakes. Please double-check responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:31:11,464 - DDQN_Agent - INFO - Starting training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train for specified number of episodes\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate training time\u001b[39;00m\n\u001b[1;32m      9\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/AI-Agent-for-Cyber-Security/src/agents/ddqn_agent.py:364\u001b[0m, in \u001b[0;36mDDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Take action and observe next state and reward\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Handle different gym versions\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/AI-Agent-for-Cyber-Security/src/gym-idsgame/gym_idsgame/envs/idsgame_env.py:124\u001b[0m, in \u001b[0;36mIdsGameEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgame_step \u001b[38;5;241m>\u001b[39m constants\u001b[38;5;241m.\u001b[39mGAME_CONFIG\u001b[38;5;241m.\u001b[39mMAX_GAME_STEPS:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mconstants\u001b[38;5;241m.\u001b[39mGAME_CONFIG\u001b[38;5;241m.\u001b[39mNEGATIVE_REWARD,\n\u001b[1;32m    122\u001b[0m                                     \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mconstants\u001b[38;5;241m.\u001b[39mGAME_CONFIG\u001b[38;5;241m.\u001b[39mNEGATIVE_REWARD), \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, info\n\u001b[0;32m--> 124\u001b[0m attack_action, defense_action \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# 1. Interpret attacker action\u001b[39;00m\n\u001b[1;32m    127\u001b[0m attacker_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mattacker_pos\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train for specified number of episodes\n",
    "train_result = agent.train()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation with more episodes\n",
    "print(\"Performing final evaluation...\")\n",
    "agent.config.eval_episodes = 100  # Increase number of evaluation episodes\n",
    "eval_result = agent.eval_model()\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_reward = sum(eval_result.episode_rewards) / len(eval_result.episode_rewards)\n",
    "avg_steps = sum(eval_result.episode_steps) / len(eval_result.episode_steps)\n",
    "\n",
    "print(f\"Final Evaluation Results:\")\n",
    "print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"Average Steps: {avg_steps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(agent.train_result.episode_rewards)\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot running average of rewards\n",
    "plt.subplot(1, 2, 2)\n",
    "window_size = 100\n",
    "running_avg = [np.mean(agent.train_result.episode_rewards[max(0, i-window_size):i+1]) \n",
    "               for i in range(len(agent.train_result.episode_rewards))]\n",
    "plt.plot(running_avg)\n",
    "plt.title(f'Running Average Reward (Window Size: {window_size})')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(result_dir, 'training_rewards.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot action distribution during training\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get action counts\n",
    "actions = list(agent.train_result.action_counts.keys())\n",
    "counts = list(agent.train_result.action_counts.values())\n",
    "\n",
    "# Sort by action index\n",
    "action_counts = sorted(zip(actions, counts))\n",
    "actions, counts = zip(*action_counts)\n",
    "\n",
    "# Plot action distribution\n",
    "plt.bar(actions, counts)\n",
    "plt.title('Action Distribution During Training')\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, axis='y')\n",
    "plt.savefig(os.path.join(result_dir, 'action_distribution.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
